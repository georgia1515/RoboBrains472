{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import csv\n",
    "import random\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "def select_answer(model, question, choice1, choice2, choice3, choice4):\n",
    "    choices = [choice1, choice2, choice3, choice4]\n",
    "    if choice1 in model:\n",
    "        choice1_similarity = model.similarity(question, choice1)\n",
    "    else:\n",
    "        choice1_similarity = 0\n",
    "    \n",
    "    if choice2 in model:\n",
    "        choice2_similarity = model.similarity(question, choice2)\n",
    "    else:\n",
    "        choice2_similarity = 0\n",
    "    \n",
    "    if choice3 in model:\n",
    "        choice3_similarity = model.similarity(question, choice3)\n",
    "    else:\n",
    "        choice3_similarity = 0\n",
    "    \n",
    "    if choice4 in model:\n",
    "        choice4_similarity = model.similarity(question, choice4)\n",
    "    else:\n",
    "        choice4_similarity = 0\n",
    "    \n",
    "    similarity = [choice1_similarity, choice2_similarity, choice3_similarity, choice4_similarity]\n",
    "    \n",
    "    answer = similarity.index(max(similarity))\n",
    "    final_choice = choices[answer]\n",
    "    return final_choice\n",
    "\n",
    "def random_guess(choice1, choice2, choice3, choice4):\n",
    "    choices = [choice1, choice2, choice3, choice4]\n",
    "    return choices[random.randint(0,3)]\n",
    "\n",
    "def create_model_details(model_name):\n",
    "    with open(model_name, 'w+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        field = ['question', 'answer', 'guess', 'label']\n",
    "        writer.writerow(field)\n",
    "\n",
    "def test_model(model_name, model, questions, answers, choice1, choice2, choice3, choice4):\n",
    "    correct_guesses = 0\n",
    "    random_guesses = 0\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        question = questions[i]\n",
    "        answer = answers[i]\n",
    "        \n",
    "        if question in model and (choice1[i] in model or choice2[i] in model or choice3[i] in model or choice4[i] in model):\n",
    "            choice = select_answer(model, question, choice1[i], choice2[i], choice3[i], choice4[i])\n",
    "            if choice == answer:\n",
    "                correct_guesses += 1\n",
    "                label = 'correct'\n",
    "            else:\n",
    "                label = 'wrong'\n",
    "        else: \n",
    "            choice = random_guess(choice1[i], choice2[i], choice3[i], choice4[i])\n",
    "            random_guesses += 1\n",
    "            label = 'guess'\n",
    "        \n",
    "        with open(model_name, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            row = [question, answer, choice, label]\n",
    "            writer.writerow(row)\n",
    "    return correct_guesses, random_guesses\n",
    "\n",
    "def write_model_analysis(model_name, size_of_vocab, number_of_correct, number_of_non_random_guess, accuracy):\n",
    "    with open('analysis.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        row = [model_name, size_of_vocab, number_of_correct, number_of_non_random_guess, accuracy]\n",
    "        writer.writerow(row)\n",
    "\n",
    "def getAnalytics(model_name, model, correct_guesses, random_guesses, questions):\n",
    "    size = len(model.key_to_index)\n",
    "    number_of_correct = correct_guesses\n",
    "    number_of_non_random_guess = len(questions) - random_guesses\n",
    "    accuracy = correct_guesses / number_of_non_random_guess\n",
    "    write_model_analysis(model_name, size, number_of_correct, number_of_non_random_guess, accuracy)\n",
    "\n",
    "# Analysis\n",
    "with open('analysis.csv', 'w+') as f:\n",
    "    writer = csv.writer(f)\n",
    "    field = ['model_name', 'size_of_vocab', 'number_of_correct', 'number_of_non_random_guess', 'accuracy']\n",
    "    writer.writerow(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec gooogle news model\n",
    "wv_google = api.load('word2vec-google-news-300')\n",
    "file_name_0 = 'word2vec-google-news-300-details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "df = pd.read_csv('synonym.csv')\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "choice1 = df['0'].tolist()\n",
    "choice2 = df['1'].tolist()\n",
    "choice3 = df['2'].tolist()\n",
    "choice4 = df['3'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "human_answer = pd.read_csv('COMP-472-per-annotator.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Print the dataframe\n",
    "# print(human_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model guessing the correct answers\n",
    "create_model_details(file_name_0)\n",
    "correct_guesses_google300, random_guesses_google300 = test_model(file_name_0, wv_google, questions, answers, choice1, choice2, choice3, choice4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAnalytics('word2vec-google-news-300', wv_google, correct_guesses_google300, random_guesses_google300, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Comparison with other pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 2 new models from different corpora but same embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove-twitter-200 <br>\n",
    "glove-wiki-gigaword-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter200 = api.load('glove-twitter-200')\n",
    "file_name_1 = 'glove-twitter-200-details.csv'\n",
    "\n",
    "glove_wiki200 = api.load('glove-wiki-gigaword-200')\n",
    "file_name_2 = 'glove-wiki-gigaword-200-details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove twitter 200 model guessing and analytics\n",
    "create_model_details(file_name_1)\n",
    "correct_guesses_twitter200, random_guesses_twitter200 = test_model(file_name_1, glove_twitter200, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-twitter-200', glove_twitter200, correct_guesses_twitter200, random_guesses_twitter200, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove wiki 200 model guessing and analytics\n",
    "create_model_details(file_name_2)\n",
    "correct_guesses_wiki200, random_guesses_wiki200 = test_model(file_name_2, glove_wiki200, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-wiki-200', glove_wiki200, correct_guesses_wiki200, random_guesses_wiki200, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 2 new models with different embdedding size but same corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove-twitter-50 <br>\n",
    "glove-twitter-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n",
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_twitter50 = api.load('glove-twitter-50')\n",
    "file_name_3 = 'glove-twitter-50-details.csv'\n",
    "\n",
    "glove_twitter100 = api.load('glove-twitter-100')\n",
    "file_name_4 = 'glove-twitter-100-details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove twitter 50 model guessing and analytics\n",
    "create_model_details(file_name_3)\n",
    "correct_guesses_twitter50, random_guesses_twitter50 = test_model(file_name_3, glove_twitter50, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-twitter-50', glove_twitter50, correct_guesses_twitter50, random_guesses_twitter50, questions)\n",
    "\n",
    "# Glove twitter 100 model guessing and analytics\n",
    "create_model_details(file_name_4)\n",
    "correct_guesses_twitter100, random_guesses_twitter100 = test_model(file_name_4, glove_twitter100, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-twitter-100', glove_twitter100, correct_guesses_twitter100, random_guesses_twitter100, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Train own models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aykch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book11_warandpeace.txt', 'book1_theakkracase.txt', 'book2_aliceinwonderland.txt', 'book3_thepictureofdoriangray.txt', 'book4_theadventuresofsherlockholmes.txt', 'book5_thegreatgatsby.txt', 'book6_modestproposal.txt', 'book7_metamorphosis.txt', 'book8_williamshakespeare.txt', 'book9_winniethepooh.txt']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "books_folder = 'books/'\n",
    "all_books = os.listdir(books_folder)\n",
    "print(all_books)\n",
    "book_sentences = {}\n",
    "\n",
    "for book in all_books:\n",
    "    book_path = os.path.join(books_folder, book)\n",
    "    with open(book_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        # print(sentences[0])\n",
    "        book_sentences[book] = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "\n",
    "flat_list = [item for sublist in book_sentences.values() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different window size 5 and 10\n",
    "model_5_100 = Word2Vec(flat_list, window=5, vector_size=100, workers=4)\n",
    "model_10_100 = Word2Vec(flat_list, window=10, vector_size=100, workers=4)\n",
    "# print(model_5_100.wv.key_to_index)\n",
    "# print(len(model_5_100.wv.key_to_index))\n",
    "\n",
    "file_name_5_100 = 'own_corpus_5_100-details.csv'\n",
    "file_name_10_100 = 'own_corpus_10_100-details.csv'\n",
    "\n",
    "# Own corpus window size 5 and 10 model guessing and analytics\n",
    "# Corpus 5 100\n",
    "create_model_details(file_name_5_100)\n",
    "correct_guesses_5_100, random_guesses_5_100 = test_model(file_name_5_100, model_5_100.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_5_100', model_5_100.wv, correct_guesses_5_100, random_guesses_5_100, questions)\n",
    "\n",
    "#  Corpus 10 100\n",
    "create_model_details(file_name_10_100)\n",
    "correct_guesses_10_100, random_guesses_10_100 = test_model(file_name_10_100, model_10_100.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_10_100', model_10_100.wv, correct_guesses_10_100, random_guesses_10_100, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different embedding size 100 and 200\n",
    "model_10_300 = Word2Vec(flat_list, window=10, vector_size=300, workers=4)\n",
    "model_10_200 = Word2Vec(flat_list, window=10, vector_size=200, workers=4)\n",
    "\n",
    "file_name_10_300 = 'own_corpus_10_300-details.csv'\n",
    "file_name_10_200 = 'own_corpus_10_200-details.csv'\n",
    "\n",
    "# Own corpus embedding size 300 and 200 model guessing and analytics\n",
    "# Corpus 10 300\n",
    "create_model_details(file_name_10_300)\n",
    "correct_guesses_10_300, random_guesses_10_300 = test_model(file_name_10_300, model_10_300.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_10_300', model_10_300.wv, correct_guesses_10_300, random_guesses_10_300, questions)\n",
    "\n",
    "# Corpus 10 200\n",
    "create_model_details(file_name_10_200)\n",
    "correct_guesses_10_200, random_guesses_10_200 = test_model(file_name_10_200, model_10_200.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_10_200', model_10_200.wv, correct_guesses_10_200, random_guesses_10_200, questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
