{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import csv\n",
    "import random\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "def select_answer(model, question, choice1, choice2, choice3, choice4):\n",
    "    choices = [choice1, choice2, choice3, choice4]\n",
    "    if choice1 in model:\n",
    "        choice1_similarity = model.similarity(question, choice1)\n",
    "    else:\n",
    "        choice1_similarity = 0\n",
    "    \n",
    "    if choice2 in model:\n",
    "        choice2_similarity = model.similarity(question, choice2)\n",
    "    else:\n",
    "        choice2_similarity = 0\n",
    "    \n",
    "    if choice3 in model:\n",
    "        choice3_similarity = model.similarity(question, choice3)\n",
    "    else:\n",
    "        choice3_similarity = 0\n",
    "    \n",
    "    if choice4 in model:\n",
    "        choice4_similarity = model.similarity(question, choice4)\n",
    "    else:\n",
    "        choice4_similarity = 0\n",
    "    \n",
    "    similarity = [choice1_similarity, choice2_similarity, choice3_similarity, choice4_similarity]\n",
    "    \n",
    "    answer = similarity.index(max(similarity))\n",
    "    final_choice = choices[answer]\n",
    "    return final_choice\n",
    "\n",
    "def random_guess(choice1, choice2, choice3, choice4):\n",
    "    choices = [choice1, choice2, choice3, choice4]\n",
    "    return choices[random.randint(0,3)]\n",
    "\n",
    "def create_model_details(model_name):\n",
    "    with open(model_name, 'w+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        field = ['question', 'answer', 'guess', 'label']\n",
    "        writer.writerow(field)\n",
    "\n",
    "def test_model(model_name, model, questions, answers, choice1, choice2, choice3, choice4):\n",
    "    correct_guesses = 0\n",
    "    random_guesses = 0\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        question = questions[i]\n",
    "        answer = answers[i]\n",
    "        \n",
    "        if question in model and (choice1[i] in model or choice2[i] in model or choice3[i] in model or choice4[i] in model):\n",
    "            choice = select_answer(model, question, choice1[i], choice2[i], choice3[i], choice4[i])\n",
    "            if choice == answer:\n",
    "                correct_guesses += 1\n",
    "                label = 'correct'\n",
    "            else:\n",
    "                label = 'wrong'\n",
    "        else: \n",
    "            choice = random_guess(choice1[i], choice2[i], choice3[i], choice4[i])\n",
    "            random_guesses += 1\n",
    "            label = 'guess'\n",
    "        \n",
    "        with open(model_name, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            row = [question, answer, choice, label]\n",
    "            writer.writerow(row)\n",
    "    return correct_guesses, random_guesses\n",
    "\n",
    "def write_model_analysis(model_name, size_of_vocab, number_of_correct, number_of_non_random_guess, accuracy):\n",
    "    with open('analysis.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        row = [model_name, size_of_vocab, number_of_correct, number_of_non_random_guess, accuracy]\n",
    "        writer.writerow(row)\n",
    "\n",
    "def getAnalytics(model_name, model, correct_guesses, random_guesses, questions):\n",
    "    size = len(model.key_to_index)\n",
    "    number_of_correct = correct_guesses\n",
    "    number_of_non_random_guess = len(questions) - random_guesses\n",
    "    accuracy = correct_guesses / number_of_non_random_guess\n",
    "    write_model_analysis(model_name, size, number_of_correct, number_of_non_random_guess, accuracy)\n",
    "\n",
    "# Analysis\n",
    "with open('analysis.csv', 'w+') as f:\n",
    "    writer = csv.writer(f)\n",
    "    field = ['model_name', 'size_of_vocab', 'number_of_correct', 'number_of_non_random_guess', 'accuracy']\n",
    "    writer.writerow(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec gooogle news model\n",
    "wv_google = api.load('word2vec-google-news-300')\n",
    "file_name_0 = 'word2vec-google-news-300-details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "# df = pd.read_csv('synonym.csv')\n",
    "# questions = df['question'].tolist()\n",
    "# answers = df['answer'].tolist()\n",
    "\n",
    "# choice1 = df['0'].tolist()\n",
    "# choice2 = df['1'].tolist()\n",
    "# choice3 = df['2'].tolist()\n",
    "# choice4 = df['3'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32md:\\GitHub\\AI-Wargame\\Word2Vec\\word2vec.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI-Wargame/Word2Vec/word2vec.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI-Wargame/Word2Vec/word2vec.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Read the CSV file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GitHub/AI-Wargame/Word2Vec/word2vec.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m human_answer \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mCOMP-472-per-annotator.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI-Wargame/Word2Vec/word2vec.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Print the dataframe\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI-Wargame/Word2Vec/word2vec.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(human_answer)\n",
      "File \u001b[1;32mc:\\Users\\rck20\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\rck20\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\rck20\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\rck20\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[0;32m   1724\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rck20\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "# human_answer = pd.read_csv('COMP-472-per-annotator.csv')\n",
    "\n",
    "# Print the dataframe\n",
    "# print(human_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model guessing the correct answers\n",
    "create_model_details(file_name_0)\n",
    "correct_guesses_google300, random_guesses_google300 = test_model(file_name_0, wv_google, questions, answers, choice1, choice2, choice3, choice4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAnalytics('word2vec-google-news-300', wv_google, correct_guesses_google300, random_guesses_google300, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Comparison with other pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 2 new models from different corpora but same embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove-twitter-200 <br>\n",
    "glove-wiki-gigaword-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter200 = api.load('glove-twitter-200')\n",
    "file_name_1 = 'glove-twitter-200-details.csv'\n",
    "\n",
    "glove_wiki200 = api.load('glove-wiki-gigaword-200')\n",
    "file_name_2 = 'glove-wiki-gigaword-200-details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove twitter 200 model guessing and analytics\n",
    "create_model_details(file_name_1)\n",
    "correct_guesses_twitter200, random_guesses_twitter200 = test_model(file_name_1, glove_twitter200, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-twitter-200', glove_twitter200, correct_guesses_twitter200, random_guesses_twitter200, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove wiki 200 model guessing and analytics\n",
    "create_model_details(file_name_2)\n",
    "correct_guesses_wiki200, random_guesses_wiki200 = test_model(file_name_2, glove_wiki200, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-wiki-200', glove_wiki200, correct_guesses_wiki200, random_guesses_wiki200, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 2 new models with different embdedding size but same corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove-twitter-50 <br>\n",
    "glove-twitter-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter50 = api.load('glove-twitter-50')\n",
    "file_name_3 = 'glove-twitter-50-details.csv'\n",
    "\n",
    "glove_twitter100 = api.load('glove-twitter-100')\n",
    "file_name_4 = 'glove-twitter-100-details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove twitter 50 model guessing and analytics\n",
    "create_model_details(file_name_3)\n",
    "correct_guesses_twitter50, random_guesses_twitter50 = test_model(file_name_3, glove_twitter50, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-twitter-50', glove_twitter50, correct_guesses_twitter50, random_guesses_twitter50, questions)\n",
    "\n",
    "# Glove twitter 100 model guessing and analytics\n",
    "create_model_details(file_name_4)\n",
    "correct_guesses_twitter100, random_guesses_twitter100 = test_model(file_name_4, glove_twitter100, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('glove-twitter-100', glove_twitter100, correct_guesses_twitter100, random_guesses_twitter100, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Train own models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rck20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book11_warandpeace.txt', 'book1_theakkracase.txt', 'book2_aliceinwonderland.txt', 'book3_thepictureofdoriangray.txt', 'book4_theadventuresofsherlockholmes.txt', 'book5_thegreatgatsby.txt', 'book6_modestproposal.txt', 'book7_metamorphosis.txt', 'book8_williamshakespeare.txt', 'book9_winniethepooh.txt']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "books_folder = 'books/'\n",
    "all_books = os.listdir(books_folder)\n",
    "print(all_books)\n",
    "# books = ['book1_theakkracase.txt', 'book2_aliceinwonderland.txt', 'book3_thepictureofdoriangray.txt', 'book4_theadventuresofsherlockholmes.txt', 'book5_thegreatgatsby.txt', 'book6_modestproposal.txt', 'book7_metamorphosis.txt', 'book8_williamshakespeare.txt', 'book9_winniethepooh.txt', 'book10_warandpeace.txt']\n",
    "book_sentences = {}\n",
    "\n",
    "for book in all_books:\n",
    "    book_path = os.path.join(books_folder, book)\n",
    "    with open(book_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        # print(sentences[0])\n",
    "        book_sentences[book] = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "\n",
    "flat_list = [item for sublist in book_sentences.values() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different window size 5 and 10\n",
    "model_5_100 = Word2Vec(flat_list, window=5, vector_size=100, workers=4)\n",
    "model_10_100 = Word2Vec(flat_list, window=10, vector_size=100, workers=4)\n",
    "# print(model_5_100.wv.key_to_index)\n",
    "# print(len(model_5_100.wv.key_to_index))\n",
    "\n",
    "file_name_5_100 = 'own_corpus_5_100.csv'\n",
    "file_name_10_100 = 'own_corpus_10_100.csv'\n",
    "\n",
    "# Own corpus window size 5 and 10 model guessing and analytics\n",
    "# Corpus 5 100\n",
    "create_model_details(file_name_5_100)\n",
    "correct_guesses_5_100, random_guesses_5_100 = test_model(file_name_5_100, model_5_100.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_5_100', model_5_100.wv, correct_guesses_5_100, random_guesses_5_100, questions)\n",
    "\n",
    "#  Corpus 10 100\n",
    "create_model_details(file_name_10_100)\n",
    "correct_guesses_10_100, random_guesses_10_100 = test_model(file_name_10_100, model_10_100.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_10_100', model_10_100.wv, correct_guesses_10_100, random_guesses_10_100, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different embedding size 100 and 200\n",
    "model_10_300 = Word2Vec(flat_list, window=10, vector_size=300, workers=4)\n",
    "model_10_200 = Word2Vec(flat_list, window=10, vector_size=200, workers=4)\n",
    "\n",
    "file_name_10_300 = 'own_corpus_10_300.csv'\n",
    "file_name_10_200 = 'own_corpus_10_200.csv'\n",
    "\n",
    "# Own corpus embedding size 300 and 200 model guessing and analytics\n",
    "# Corpus 10 300\n",
    "create_model_details(file_name_10_300)\n",
    "correct_guesses_10_300, random_guesses_10_300 = test_model(file_name_10_300, model_10_300.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_10_300', model_10_300.wv, correct_guesses_10_300, random_guesses_10_300, questions)\n",
    "\n",
    "# Corpus 10 200\n",
    "create_model_details(file_name_10_200)\n",
    "correct_guesses_10_200, random_guesses_10_200 = test_model(file_name_10_200, model_10_200.wv, questions, answers, choice1, choice2, choice3, choice4)\n",
    "getAnalytics('own_corpus_10_200', model_10_200.wv, correct_guesses_10_200, random_guesses_10_200, questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
