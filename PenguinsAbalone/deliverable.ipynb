{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Experiments with Machine Learning\n",
    "Alice Chen\n",
    "Georgia Pitic\n",
    "Ryan Kim\n",
    "\n",
    "Path: deliverable.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPenguins = pd.read_csv('penguins.csv')\n",
    "dataAbalone = pd.read_csv('abalone.csv')\n",
    "\n",
    "# a. Penguin dataset: Methods of convertion\n",
    "# i. Convert dummy coded data\n",
    "# dataPenguins = pd.get_dummies(dataPenguins, columns=['island', 'sex'])\n",
    "# dataPenguins.to_csv('penguins_with_dummies.csv', index=False)\n",
    "# dataPenguinsWithDummy = pd.read_csv('penguins_with_dummies.csv')\n",
    "\n",
    "# ii. Convert categorical data\n",
    "le = LabelEncoder()\n",
    "dataPenguins['island'] = le.fit_transform(dataPenguins['island'])\n",
    "dataPenguins['sex'] = le.fit_transform(dataPenguins['sex'])\n",
    "\n",
    "dataPenguins.to_csv('penguins_with_dummies.csv', index=False)\n",
    "dataPenguinsWithDummy = pd.read_csv('penguins_with_dummies.csv')\n",
    "\n",
    "# b. Abalone dataset: Methods of convertion\n",
    "#  Do not need to convert data into numerical values since all data is numerical, except the GOAL (OK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot the percentages of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penguins plot percentage\n",
    "dataPenguinsPercentage = (dataPenguinsWithDummy['species'].value_counts(normalize=True) / len(dataPenguinsWithDummy)) * 100\n",
    "\n",
    "dataPenguinsPercentage.plot(kind='bar')\n",
    "plt.title('Penguins Species Percentage')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('penguinsPercentage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abalone plot percentage\n",
    "dataAbalonePercentage = (dataAbalone['Type'].value_counts(\n",
    "    normalize=True) / len(dataAbalone)) * 100\n",
    "\n",
    "dataAbalonePercentage.plot(kind='bar')\n",
    "plt.title('Abalone Sex Percentage')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('abalonePercentage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From analysis of dataset: classes are more balanced for abalone than penguins' dataset, as the percentage of abalone's classes are more approximately the same. \n",
    "- Since the classes are more balanced for abalone, accuracy is more appropriate to use to evaluate the performance. \n",
    "- Since the classes are less balanced for penguins, precision, recall, and F1 score are more appropriate to use to evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split dataset using train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Penguins\n",
    "X_penguins = dataPenguinsWithDummy.drop(['species'], axis=1)\n",
    "y_penguins = dataPenguinsWithDummy['species']\n",
    "X_penguinsTrain, X_penguinsTest, y_penguinsTrain, y_penguinsTest = train_test_split(X_penguins, y_penguins)\n",
    "\n",
    "# b. Abalone\n",
    "X_abalone = dataAbalone.drop(['Type'], axis=1)\n",
    "y_abalone = dataAbalone['Type']\n",
    "X_abaloneTrain, X_abaloneTest, y_abaloneTrain, y_abaloneTest = train_test_split(X_abalone, y_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train and test 4 different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a Base DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_penguins = tree.DecisionTreeClassifier()\n",
    "\n",
    "# i. Penguins\n",
    "dtc_penguins.fit(X_penguinsTrain, y_penguinsTrain)\n",
    "tree.plot_tree(dtc_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_abalone = tree.DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# ii. Abalone\n",
    "dtc_abalone.fit(X_abaloneTrain, y_abaloneTrain)\n",
    "tree.plot_tree(dtc_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b Top DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [4, 6, None],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "}\n",
    "\n",
    "# i. Penguins\n",
    "topDTC_penguins = tree.DecisionTreeClassifier()\n",
    "grid_search_penguins = GridSearchCV(estimator=topDTC_penguins, param_grid=dt_params_grid)\n",
    "grid_search_penguins.fit(X_penguinsTrain, y_penguinsTrain)\n",
    "best_dt_classifier_penguins = grid_search_penguins.best_estimator_\n",
    "tree.plot_tree(best_dt_classifier_penguins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Abalone\n",
    "\n",
    "topDTC_abalone = tree.DecisionTreeClassifier(max_depth=4)\n",
    "grid_search_abalone = GridSearchCV(estimator=topDTC_abalone, param_grid=dt_params_grid)\n",
    "grid_search_abalone.fit(X_abaloneTrain, y_abaloneTrain)\n",
    "best_dt_classifier_abalone = grid_search_abalone.best_estimator_\n",
    "tree.plot_tree(best_dt_classifier_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c Base MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. Penguins\n",
    "mlp_penguins = MLPClassifier(hidden_layer_sizes=(100, 100) ,activation='logistic', solver='sgd')\n",
    "\n",
    "mlp_penguins.fit(X_penguinsTrain, y_penguinsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Abalone\n",
    "mlp_abalone = MLPClassifier(hidden_layer_sizes=(100, 100) ,activation='logistic', solver='sgd')\n",
    "\n",
    "mlp_abalone.fit(X_abaloneTrain, y_abaloneTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4d Top MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. Penguins\n",
    "mlp_param_grid = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "}\n",
    "top_mlp_penguins = MLPClassifier()\n",
    "\n",
    "grid_search_mlp_penguins = GridSearchCV(estimator=top_mlp_penguins, param_grid=mlp_param_grid)\n",
    "grid_search_mlp_penguins.fit(X_penguinsTrain, y_penguinsTrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Abalone\n",
    "top_mlp_abalone = MLPClassifier()\n",
    "\n",
    "grid_search_mlp_abalone = GridSearchCV(estimator=top_mlp_abalone, param_grid=mlp_param_grid)\n",
    "grid_search_mlp_abalone.fit(X_abaloneTrain, y_abaloneTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Find file to write metric results\n",
    "file_name_penguins = 'penguin-performance.txt'\n",
    "file_name_abalone = 'abalone-performance.txt'\n",
    "\n",
    "if os.path.exists(file_name_penguins):\n",
    "    with open(file_name_penguins, 'w') as file:\n",
    "        file.seek(0)\n",
    "        file.truncate()\n",
    "        file.close()\n",
    "\n",
    "if os.path.exists(file_name_abalone):\n",
    "    with open(file_name_abalone, 'w') as file:\n",
    "        file.seek(0)\n",
    "        file.truncate()\n",
    "        file.close()\n",
    "\n",
    "# Function to evaluate each model\n",
    "def evaluate_model(title, classifier, X_train, y_train, X_test, y_test, file_name):\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(file_name, 'a') as file:\n",
    "        # (A) Model Description\n",
    "        file.write(f\"\\n{'_' * 80}\\n\")\n",
    "        file.write(f\"(A)\\n\")\n",
    "        file.write(f\"Model: {title}\\n\")\n",
    "        if (title == 'Base-DT Abalone'):\n",
    "            params = f\"\"\"Hyperparameters: {{'max_depth': {classifier.get_params()['max_depth']}}}\\n\"\"\"\n",
    "            file.write(params)\n",
    "        elif (title == 'Top-DT Penguins' or title == 'Top-DT Abalone'):\n",
    "            file.write(f\"Hyperparameters modified: {dt_params_grid}\\n\")\n",
    "            best_params = f\"\"\"Best hyperparameters: {{'criterion': {classifier.get_params()['criterion']}, 'max_depth': {classifier.get_params()['max_depth']}, 'min_samples_split': {classifier.get_params()['min_samples_split']}}}\\n\"\"\"\n",
    "            file.write(best_params)\n",
    "        elif (title == 'Base-MLP Penguins' or title == 'Base-MLP Abalone' ):\n",
    "            params = f\"\"\"Hyperparameters modified: {{'hidden_layer_sizes': {classifier.get_params()['hidden_layer_sizes']}, 'activation': {classifier.get_params()['activation']}, 'solver': {classifier.get_params()['solver']}}}\\n\"\"\"\n",
    "            file.write(params)\n",
    "        elif (title == 'Top-MLP Penguins' or title == 'Top-MLP Abalone'):\n",
    "            file.write(f\"Hyperparameters modified: {mlp_param_grid}\\n\") \n",
    "            best_params = f\"\"\"Best hyperparameters: {{'activation': {classifier.get_params()['activation']}, 'hidden_layer_sizes': {classifier.get_params()['hidden_layer_sizes']}, 'solver': {classifier.get_params()['solver']}}}\\n\"\"\"\n",
    "            file.write(best_params)\n",
    "            \n",
    "        # (B) Confusion Matrix\n",
    "        file.write(f\"\\n(B)\\n\")\n",
    "        file.write(\"Confusion Matrix:\\n\")\n",
    "        file.write(f\"{np.array2string(cm)}\\n\")\n",
    "        \n",
    "        # (C) Classification Report: Precision, recall, and F1-measure\n",
    "        file.write(f\"\\n(C)\\n\")\n",
    "        file.write(\"Classification Report:\\n\")\n",
    "        file.write(f\"{classification_report(y_test, y_pred)}\\n\")\n",
    "        \n",
    "        # (D) Accuracy, macro-average F1 and weighted-average F1\n",
    "        file.write(f\"\\n(D)\\n\")\n",
    "        file.write(f\"Accuracy: {accuracy:.9f}\\n\")\n",
    "        file.write(f\"Macro-average F1: {f1_macro:.9f}\\n\")\n",
    "        file.write(f\"Weighted-average F1: {f1_weighted:.9f}\\n\")\n",
    "        file.write(f\"{'_' * 80}\\n\")\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-DT\n",
    "evaluate_model('Base-DT Penguins', dtc_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)\n",
    "\n",
    "# Top-DT\n",
    "evaluate_model('Top-DT Penguins', grid_search_penguins.best_estimator_, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)\n",
    "\n",
    "# Base-MLP\n",
    "evaluate_model('Base-MLP Penguins', mlp_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)\n",
    "\n",
    "# Top-MLP\n",
    "evaluate_model('Top-MLP Penguins', grid_search_mlp_penguins.best_estimator_, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-DT\n",
    "evaluate_model('Base-DT Abalone', dtc_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)\n",
    "\n",
    "# Top-DT\n",
    "evaluate_model('Top-DT Abalone', grid_search_abalone.best_estimator_, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)\n",
    "\n",
    "# Base-MLP\n",
    "evaluate_model('Base-MLP Abalone', mlp_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)\n",
    "\n",
    "# Top-MLP\n",
    "evaluate_model('Top-MLP Abalone', grid_search_mlp_abalone.best_estimator_, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Average performance for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Average performance for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run 5 times step 4-5 \n",
    "def run_experiments(model, X_train, y_train, X_test, y_test):\n",
    "    accuracies = []\n",
    "    macro_f1s = []\n",
    "    weighted_f1s = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        # Train and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        # Append metrics values to metric lists\n",
    "        accuracies.append(accuracy)\n",
    "        macro_f1s.append(report['macro avg']['f1-score'])\n",
    "        weighted_f1s.append(report['weighted avg']['f1-score'])\n",
    "        \n",
    "    # Calculate average and variance\n",
    "    accuracy_mean, accuracy_var = np.mean(accuracies), np.var(accuracies)\n",
    "    macro_f1_mean, macro_f1_var = np.mean(macro_f1s), np.var(macro_f1s)\n",
    "    weighted_f1_mean, weighted_f1_var = np.mean(weighted_f1s), np.var(weighted_f1s)\n",
    "    \n",
    "    return {\n",
    "        'accuracy_mean': accuracy_mean, 'accuracy_var': accuracy_var,\n",
    "        'macro_f1_mean': macro_f1_mean, 'macro_f1_var': macro_f1_var,\n",
    "        'weighted_f1_mean': weighted_f1_mean, 'weighted_f1_var': weighted_f1_var\n",
    "    }\n",
    "\n",
    "# Function to append metrics results to file \n",
    "def append_to_file(filename, model_name, results):\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f'\\n--- {model_name} ---\\n')\n",
    "        for metric, value in results.items():\n",
    "            file.write(f'{metric}: {value:.9f}\\n')\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penguins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penguins\n",
    "\n",
    "# Base-DT\n",
    "results = run_experiments(dtc_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Base-DT', results)\n",
    "\n",
    "# Top-DT\n",
    "best_dt_penguins = grid_search_penguins.best_estimator_\n",
    "\n",
    "penguin_metrics = run_experiments(best_dt_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Top-DT', penguin_metrics)\n",
    "\n",
    "# Base-MLP\n",
    "penguin_metrics = run_experiments(mlp_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Base-MLP', penguin_metrics)\n",
    "\n",
    "# Top-MLP\n",
    "best_mlp_penguins = grid_search_mlp_penguins.best_estimator_\n",
    "\n",
    "penguin_metrics = run_experiments(best_mlp_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Top-MLP', penguin_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abalone\n",
    "\n",
    "# Base-DT\n",
    "results = run_experiments(dtc_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Base-DT', results)\n",
    "\n",
    "# Top-DT\n",
    "best_dt_abalone = grid_search_abalone.best_estimator_\n",
    "\n",
    "abalone_metrics = run_experiments(best_dt_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Top-DT', abalone_metrics)\n",
    "\n",
    "# Base-MLP\n",
    "abalone_metrics = run_experiments(mlp_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Base-MLP', abalone_metrics)\n",
    "\n",
    "# Top-MLP for Abalone\n",
    "best_mlp_abalone = grid_search_mlp_abalone.best_estimator_\n",
    "\n",
    "abalone_metrics = run_experiments(best_mlp_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Top-MLP', abalone_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the same model, the performance changes every time since the data used to train and test the model is randomly split into training and testing sets.\n",
    "- Since the variance is significantly lower than the mean, the standard deviation is likely to be low as well, as the standard deviation is the square root of the variance. \n",
    "- A low standard deviation indicates that the performance of the model is relatively stable and consistent, meaning the model is likely to be stable and generalize well to data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
