{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Experiments with Machine Learning\n",
    "Alice Chen\n",
    "Georgia Pitic\n",
    "Ryan Kim\n",
    "\n",
    "Path: deliverable.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. methods of convertion\n",
    "# i. Convert dummy coded data\n",
    "\n",
    "dataAbalone = pd.read_csv('abalone.csv')\n",
    "dataPenguins = pd.read_csv('penguins.csv')\n",
    "dataPenguins = pd.get_dummies(dataPenguins, columns=['island', 'sex'])\n",
    "dataPenguins.to_csv('penguins_with_dummies.csv', index=False)\n",
    "\n",
    "dataPenguinsWithDummy = pd.read_csv('penguins_with_dummies.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot the percentages of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penguines plot percentage\n",
    "dataPenguinsPercentage = (dataPenguinsWithDummy['species'].value_counts(\n",
    "    normalize=True) / len(dataPenguinsWithDummy)) * 100\n",
    "\n",
    "dataPenguinsPercentage.plot(kind='bar')\n",
    "plt.title('Penguins Species Percentage')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('penguinsPercentage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abalone plot percentage\n",
    "dataAbalonePercentage = (dataAbalone['Type'].value_counts(\n",
    "    normalize=True) / len(dataAbalone)) * 100\n",
    "\n",
    "dataAbalonePercentage.plot(kind='bar')\n",
    "plt.title('Abalone Sex Percentage')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('abalonePercentage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split dataset using train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Penguins\n",
    "X_penguins = dataPenguinsWithDummy.drop(['species'], axis=1)\n",
    "y_penguins = dataPenguinsWithDummy['species']\n",
    "X_penguinsTrain, X_penguinsTest, y_penguinsTrain, y_penguinsTest = train_test_split(X_penguins, y_penguins)\n",
    "\n",
    "# b. Abalone\n",
    "X_abalone = dataAbalone.drop(['Type'], axis=1)\n",
    "y_abalone = dataAbalone['Type']\n",
    "X_abaloneTrain, X_abaloneTest, y_abaloneTrain, y_abaloneTest = train_test_split(X_abalone, y_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train and test 4 different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a Base DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_penguins = tree.DecisionTreeClassifier()\n",
    "dtc_abalone = tree.DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# i. Penguins\n",
    "dtc_penguins.fit(X_penguinsTrain, y_penguinsTrain)\n",
    "tree.plot_tree(dtc_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Abalone\n",
    "dtc_abalone.fit(X_abaloneTrain, y_abaloneTrain)\n",
    "tree.plot_tree(dtc_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b Top DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [4, 6, None],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "}\n",
    "\n",
    "# i. Penguins\n",
    "topDTC_penguins = tree.DecisionTreeClassifier()\n",
    "grid_search_penguins = GridSearchCV(estimator=topDTC_penguins, param_grid=params_grid)\n",
    "\n",
    "grid_search_penguins.fit(X_penguinsTrain, y_penguinsTrain)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)\n",
    "# print(grid_search.best_estimator_)\n",
    "best_dt_classifier = grid_search_penguins.best_estimator_\n",
    "\n",
    "best_dt_classifier.fit(X_penguinsTest, y_penguinsTest)\n",
    "# tree.plot_tree(grid_search.best_estimator_)\n",
    "\n",
    "# FOR LATER IN STEP 6\n",
    "# accuracy = best_dt_classifier.score(X_penguinsTrain, y_penguinsTrain)\n",
    "# print('Penguins Decision Tree Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Abalone\n",
    "\n",
    "topDTC_abalone = tree.DecisionTreeClassifier(max_depth=6)\n",
    "grid_search_abalone = GridSearchCV(estimator=topDTC_abalone, param_grid=params_grid)\n",
    "grid_search_abalone.fit(X_abaloneTrain, y_abaloneTrain)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)\n",
    "# print(grid_search.best_estimator_)\n",
    "best_dt_classifier = grid_search_abalone.best_estimator_\n",
    "\n",
    "best_dt_classifier.fit(X_abaloneTest, y_abaloneTest)\n",
    "# FOR LATER IN STEP 6\n",
    "# accuracy = best_dt_classifier.score(X_abaloneTest, y_abaloneTest)\n",
    "# print('Penguins Decision Tree Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c Base MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_penguins = MLPClassifier(hidden_layer_sizes=(100, 100) ,activation='logistic', solver='sgd')\n",
    "\n",
    "# i. Penguins\n",
    "mlp_penguins.fit(X_penguinsTrain, y_penguinsTrain)\n",
    "mlp_penguins.score(X_penguinsTest, y_penguinsTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Abalone\n",
    "mlp_abalone = MLPClassifier(hidden_layer_sizes=(100, 100) ,activation='logistic', solver='sgd')\n",
    "\n",
    "mlp_abalone.fit(X_abaloneTrain, y_abaloneTrain)\n",
    "mlp_abalone.score(X_abaloneTest, y_abaloneTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4d Top MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. TOP MLP\n",
    "# i. Penguins\n",
    "\n",
    "# Define the MLPClassifier\n",
    "mlp_param_grid = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "}\n",
    "mlp = MLPClassifier(max_iter=200)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_mlp_penguins = GridSearchCV(estimator=mlp, param_grid=mlp_param_grid)\n",
    "grid_search_mlp_penguins.fit(X_penguinsTrain, y_penguinsTrain)  # Replace with your training data variables\n",
    "\n",
    "# Output the best parameters\n",
    "# print(\"Best parameters set found on development set:\")\n",
    "# print(grid_search_mlp_penguins.best_params_)\n",
    "\n",
    "# After fitting GridSearchCV to your data\n",
    "# best_score = grid_search_mlp_penguins.best_score_\n",
    "\n",
    "# Print the best score\n",
    "# print(best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. TOP MLP\n",
    "# ii. Abalons\n",
    "\n",
    "# Define the MLPClassifier\n",
    "mlp_param_grid = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "}\n",
    "mlp = MLPClassifier(max_iter=200)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_mlp_abalone = GridSearchCV(estimator=mlp, param_grid=mlp_param_grid)\n",
    "grid_search_mlp_abalone.fit(X_abaloneTrain, y_abaloneTrain)  # Replace with your training data variables\n",
    "\n",
    "# Output the best parameters\n",
    "# print(\"Best parameters set found on development set:\")\n",
    "# print(grid_search_mlp_abalone.best_params_)\n",
    "\n",
    "# After fitting GridSearchCV to your data\n",
    "# best_score = grid_search_mlp_abalone.best_score_\n",
    "\n",
    "# Print the best score\n",
    "# print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "file_name_penguins = 'penguin-performance.txt'\n",
    "file_name_abalone = 'abalone-performance.txt'\n",
    "\n",
    "if os.path.exists(file_name_penguins):\n",
    "# If file exists, then clear its contents\n",
    "    with open(file_name_penguins, 'w') as file:\n",
    "        file.seek(0)\n",
    "        file.truncate()\n",
    "        file.close()\n",
    "\n",
    "if os.path.exists(file_name_abalone):\n",
    "    with open(file_name_abalone, 'w') as file:\n",
    "        file.seek(0)\n",
    "        file.truncate()\n",
    "        file.close()\n",
    "\n",
    "def evaluate_model(title, classifier, X_train, y_train, X_test, y_test, file_name):\n",
    "    # Fit the classifier\n",
    "    # classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    cr = classification_report(y_test, predictions, output_dict=True)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1_macro = f1_score(y_test, predictions, average='macro')\n",
    "    f1_weighted = f1_score(y_test, predictions, average='weighted')\n",
    "    \n",
    "\n",
    "    # Write to file\n",
    "    with open(file_name, 'a') as file:\n",
    "        # (A) Model Description\n",
    "        file.write(f\"\\n{'_' * 80}\\n\")\n",
    "        file.write(f\"(A)\\n\")\n",
    "        file.write(f\"Model: {title}\\n\")\n",
    "        if hasattr(classifier, 'best_params_'):\n",
    "            file.write(f\"Best Parameters: {classifier.best_params_}\\n\")\n",
    "        \n",
    "        # (B) Confusion Matrix\n",
    "        file.write(f\"\\n(B)\\n\")\n",
    "        file.write(\"Confusion Matrix:\\n\")\n",
    "        file.write(f\"{np.array2string(cm)}\\n\")\n",
    "        \n",
    "        # (C) Classification Report\n",
    "        file.write(f\"\\n(C)\\n\")\n",
    "        file.write(\"Classification Report:\\n\")\n",
    "        file.write(f\"{classification_report(y_test, predictions)}\\n\")\n",
    "        \n",
    "        # (D) Accuracy and F1 Scores\n",
    "        file.write(f\"\\n(D)\\n\")\n",
    "        file.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "        file.write(f\"Macro-average F1: {f1_macro:.4f}\\n\")\n",
    "        file.write(f\"Weighted-average F1: {f1_weighted:.4f}\\n\")\n",
    "        file.write(f\"{'_' * 80}\\n\")\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-DT\n",
    "evaluate_model('Base-DT Penguins', dtc_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)\n",
    "\n",
    "# Top-DT\n",
    "evaluate_model('Top-DT Penguins', grid_search_penguins.best_estimator_, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)\n",
    "\n",
    "# Base-MLP\n",
    "evaluate_model('Base-MLP Penguins', mlp_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)\n",
    "\n",
    "# Top-MLP\n",
    "evaluate_model('Top-MLP Penguins', grid_search_mlp_penguins.best_estimator_, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest, file_name_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-DT\n",
    "evaluate_model('Base-DT Abalone', dtc_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)\n",
    "\n",
    "# Top-DT\n",
    "evaluate_model('Top-DT Abalone', grid_search_abalone.best_estimator_, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)\n",
    "\n",
    "# Base-MLP\n",
    "evaluate_model('Base-MLP Abalone', mlp_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)\n",
    "\n",
    "# Top-MLP\n",
    "evaluate_model('Top-MLP Abalone', grid_search_mlp_abalone.best_estimator_, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest, file_name_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Average performance for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Assuming evaluate_model is a function that trains, predicts, and evaluates the model\n",
    "# and returns a dictionary with accuracy, macro F1, and weighted F1.\n",
    "\n",
    "def run_experiments(model, X_train, y_train, X_test, y_test):\n",
    "    accuracies = []\n",
    "    macro_f1s = []\n",
    "    weighted_f1s = []\n",
    "    \n",
    "    # Run 5 experiments\n",
    "    for i in range(5):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        report = classification_report(y_test, predictions, output_dict=True)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        macro_f1s.append(report['macro avg']['f1-score'])\n",
    "        weighted_f1s.append(report['weighted avg']['f1-score'])\n",
    "        \n",
    "    # Calculate the means and variances\n",
    "    accuracy_mean, accuracy_var = np.mean(accuracies), np.var(accuracies)\n",
    "    macro_f1_mean, macro_f1_var = np.mean(macro_f1s), np.var(macro_f1s)\n",
    "    weighted_f1_mean, weighted_f1_var = np.mean(weighted_f1s), np.var(weighted_f1s)\n",
    "    \n",
    "    # Return the results as a dictionary\n",
    "    return {\n",
    "        'accuracy_mean': accuracy_mean, 'accuracy_var': accuracy_var,\n",
    "        'macro_f1_mean': macro_f1_mean, 'macro_f1_var': macro_f1_var,\n",
    "        'weighted_f1_mean': weighted_f1_mean, 'weighted_f1_var': weighted_f1_var\n",
    "    }\n",
    "\n",
    "# Function to calculate the mean and variance of the evaluation metrics\n",
    "# def calculate_mean_variance(results):\n",
    "#     mean_variance = {}\n",
    "#     for key, value in results.items():\n",
    "#         mean_variance[f'{key}_mean'] = np.mean(value)\n",
    "#         mean_variance[f'{key}_var'] = np.var(value)\n",
    "#     return mean_variance\n",
    "\n",
    "def append_to_file(filename, model_name, results):\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f'\\n--- {model_name} ---\\n')\n",
    "        for metric, value in results.items():\n",
    "            file.write(f'{metric}: {value:.4f}\\n')\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penguins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penguins\n",
    "\n",
    "# Base-DT\n",
    "results = run_experiments(dtc_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Base-DT', results)\n",
    "\n",
    "# Top-DT\n",
    "best_dt_penguins = grid_search_penguins.best_estimator_\n",
    "\n",
    "penguin_metrics = run_experiments(best_dt_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Top-DT', penguin_metrics)\n",
    "\n",
    "# Base-MLP\n",
    "penguin_metrics = run_experiments(mlp_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Base-MLP', penguin_metrics)\n",
    "\n",
    "# Top-MLP\n",
    "best_mlp_penguins = grid_search_mlp_penguins.best_estimator_\n",
    "\n",
    "penguin_metrics = run_experiments(best_mlp_penguins, X_penguinsTrain, y_penguinsTrain, X_penguinsTest, y_penguinsTest)\n",
    "append_to_file(file_name_penguins, 'Top-MLP', penguin_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abalone\n",
    "\n",
    "# Base-DT\n",
    "# base_dt_abalone = tree.DecisionTreeClassifier()\n",
    "results = run_experiments(dtc_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Base-DT', results)\n",
    "\n",
    "# Top-DT\n",
    "best_dt_abalone = grid_search_abalone.best_estimator_\n",
    "\n",
    "abalone_metrics = run_experiments(best_dt_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Top-DT', abalone_metrics)\n",
    "\n",
    "# Base-MLP\n",
    "abalone_metrics = run_experiments(mlp_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Base-MLP', abalone_metrics)\n",
    "\n",
    "# Top-MLP for Abalone\n",
    "best_mlp_abalone = grid_search_mlp_abalone.best_estimator_\n",
    "\n",
    "abalone_metrics = run_experiments(best_mlp_abalone, X_abaloneTrain, y_abaloneTrain, X_abaloneTest, y_abaloneTest)\n",
    "append_to_file(file_name_abalone, 'Top-MLP', abalone_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
