
________________________________________________________________________________
(A)
Model: Base-DT Penguins

(B)
Confusion Matrix:
[[32  1  0]
 [ 0 17  0]
 [ 0  3 31]]

(C)
Report for Precision, Recall, and F1-measure:
           precision    recall  f1-score
Adelie      1.000000  0.969697  0.984615
Chinstrap   0.809524  1.000000  0.894737
Gentoo      1.000000  0.911765  0.953846

(D)
Accuracy: 0.952380952
Macro-average F1: 0.944399460
Weighted-average F1: 0.953971467
________________________________________________________________________________

________________________________________________________________________________
(A)
Model: Top-DT Penguins
Hyperparameters modified: {'criterion': ['gini', 'entropy'], 'max_depth': [4, 6, None], 'min_samples_split': [2, 4, 6]}
Best hyperparameters: {'criterion': gini, 'max_depth': 4, 'min_samples_split': 2}

(B)
Confusion Matrix:
[[32  1  0]
 [ 0 17  0]
 [ 0  3 31]]

(C)
Report for Precision, Recall, and F1-measure:
           precision    recall  f1-score
Adelie      1.000000  0.969697  0.984615
Chinstrap   0.809524  1.000000  0.894737
Gentoo      1.000000  0.911765  0.953846

(D)
Accuracy: 0.952380952
Macro-average F1: 0.944399460
Weighted-average F1: 0.953971467
________________________________________________________________________________

________________________________________________________________________________
(A)
Model: Base-MLP Penguins
Hyperparameters modified: {'hidden_layer_sizes': (100, 100), 'activation': logistic, 'solver': sgd}

(B)
Confusion Matrix:
[[33  0  0]
 [17  0  0]
 [34  0  0]]

(C)
Report for Precision, Recall, and F1-measure:
           precision  recall  f1-score
Adelie      0.392857     1.0  0.564103
Chinstrap   0.000000     0.0  0.000000
Gentoo      0.000000     0.0  0.000000

(D)
Accuracy: 0.392857143
Macro-average F1: 0.188034188
Weighted-average F1: 0.221611722
________________________________________________________________________________

________________________________________________________________________________
(A)
Model: Top-MLP Penguins
Hyperparameters modified: {'activation': ['logistic', 'tanh', 'relu'], 'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'solver': ['adam', 'sgd']}
Best hyperparameters: {'activation': logistic, 'hidden_layer_sizes': (30, 50), 'solver': adam}

(B)
Confusion Matrix:
[[28  0  5]
 [17  0  0]
 [ 6  0 28]]

(C)
Report for Precision, Recall, and F1-measure:
           precision    recall  f1-score
Adelie      0.549020  0.848485  0.666667
Chinstrap   0.000000  0.000000  0.000000
Gentoo      0.848485  0.823529  0.835821

(D)
Accuracy: 0.666666667
Macro-average F1: 0.500829187
Weighted-average F1: 0.600213220
________________________________________________________________________________

--- Base-DT ---
accuracy_mean: 0.952380952
accuracy_var: 0.000000000
macro_f1_mean: 0.944399460
macro_f1_var: 0.000000000
weighted_f1_mean: 0.953971467
weighted_f1_var: 0.000000000

--- Top-DT ---
accuracy_mean: 0.952380952
accuracy_var: 0.000000000
macro_f1_mean: 0.944399460
macro_f1_var: 0.000000000
weighted_f1_mean: 0.953971467
weighted_f1_var: 0.000000000

--- Base-MLP ---
accuracy_mean: 0.392857143
accuracy_var: 0.000000000
macro_f1_mean: 0.188034188
macro_f1_var: 0.000000000
weighted_f1_mean: 0.221611722
weighted_f1_var: 0.000000000

--- Top-MLP ---
accuracy_mean: 0.500000000
accuracy_var: 0.017233560
macro_f1_mean: 0.310094554
macro_f1_var: 0.022348388
weighted_f1_mean: 0.369421645
weighted_f1_var: 0.032772184
